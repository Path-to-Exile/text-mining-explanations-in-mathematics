{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Last preprocessing step. Removes all non-relevant articles from our corpus. Removes non-relevant information from arxiv-full-database.\n",
    "###### Adds word count for each context to the data frame\n",
    "###### Saves the result in database-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list):\n",
    "    return \"\".join([item for sublist in list for item in sublist])\n",
    "path = '/Users/Svesketerning/Google-Drev/experiments/database-files'\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv = pd.read_pickle(r'arxiv-full-database-20210427-1638.pickle')\n",
    "keep = ['title','abstract','authors_parsed',\n",
    "        'numauthors','categories_parsed','numcats','maincat',\n",
    "       'nummathcats','mainmathcat']\n",
    "keep_og = ['title','abstract','authors_parsed','numpages',\n",
    "        'numauthors','categories_parsed','numcats','maincat',\n",
    "       'nummathcats','mainmathcat']\n",
    "df_arxiv = df_arxiv[keep] # Keep only relevant columns\n",
    "\n",
    "df_arxiv['maincat'].replace({'cs.IT': 'math.IT', 'math-ph': 'math.MP'}, inplace=True)\n",
    "df_arxiv = df_arxiv.drop(df_arxiv[df_arxiv.maincat != df_arxiv.mainmathcat].index) \n",
    "# Remove articles without mainmathcat=maincat\n",
    "\n",
    "df_contexts = pd.read_feather('arxiv-contexts-20210428-1144.feather')\n",
    "contexts = [x for x in df_contexts.columns if x!='arxivid']\n",
    "for c in contexts:\n",
    "    df_contexts[c] = df_contexts[c].apply(flatten)\n",
    "df_contexts = df_contexts.set_index('arxivid')\n",
    "df_contexts.index.names = ['id'] # ArXiv id as index \n",
    "\n",
    "article_list = df_arxiv.index.tolist()\n",
    "df_contexts = df_contexts[df_contexts.index.isin(article_list)] # Drop rows if index not in df_arxiv\n",
    "# Outcome is all articles with maincat a mathematics category alongside contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count for each context to the data frame for each article\n",
    "contextwordcount_df = [] # Counting words in each context per article\n",
    "for index in range(len(df_contexts.index)):\n",
    "    counts_article = []\n",
    "    for c in contexts:\n",
    "        count = len(nltk.word_tokenize(df_contexts[c][index]))\n",
    "        counts_article.append(count)\n",
    "    contextwordcount_df.append({'id':df_contexts.index[index], # Non functional as I list each context\n",
    "                                'outer': counts_article[0], 'theorem': counts_article[1], \n",
    "                                'meta': counts_article[2], 'proof': counts_article[3], \n",
    "                                'other': counts_article[4]})\n",
    "contextwordcount_df = pd.concat([pd.DataFrame(contextwordcount_df[i], index=['id']) for i in range(len(contextwordcount_df))]) \n",
    "contextwordcount_df = contextwordcount_df.set_index('id')\n",
    "# merge to df_arxiv\n",
    "df_arxiv = df_arxiv.merge(contextwordcount_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv = df_arxiv.reset_index()\n",
    "df_contexts = df_contexts.reset_index()\n",
    "df_contexts.to_feather(path+'/arxiv_contexts'+timestr+'.feather')\n",
    "df_arxiv.to_feather(path+'/arxiv_extended_database'+timestr+'.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also checks for Ramos Corpus\n",
    "LatestDatabaseFile = max(glob.iglob('/Users/Svesketerning/Google-Drev/experiments/database-files/arxiv_extended_database*'),key=os.path.getctime)\n",
    "df_arxiv = pd.read_feather(LatestDatabaseFile)\n",
    "\n",
    "cat_df = pd.DataFrame(columns=['Count 2020','Count 2009','pct 2020','pct 2009'])\n",
    "\n",
    "for i in df_arxiv['mainmathcat'].unique().tolist():\n",
    "    values_to_add = {'Count 2020': len(df_arxiv[df_arxiv['mainmathcat']==i]),\n",
    "                     'Count 2009': len(df_arxiv_ramos[df_arxiv_ramos['mainmathcat']==i]),\n",
    "                     'pct 2020': len(df_arxiv[df_arxiv['mainmathcat']==i])/12990, \n",
    "                     'pct 2009': len(df_arxiv_ramos[df_arxiv_ramos['mainmathcat']==i])/9686\n",
    "                    }\n",
    "    row_to_add = pd.Series(values_to_add, name = i)\n",
    "    cat_df = cat_df.append(row_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance \n",
    "\n",
    "list_2020 = cat_df.sort_values(by = 'pct 2020').index.to_list()\n",
    "list_2009 = cat_df.sort_values(by = 'pct 2009').index.to_list()\n",
    "\n",
    "textdistance.jaro_winkler(list_2020,list_2009)\n",
    "sum(np.abs(cat_df['pct 2020']-cat_df['pct 2009']))/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RamosMejia2009 Cropus check \n",
    "#### using my Pipeline (removing all articles where maincat != mainmathcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ramos = pd.read_feather(r'corpus-MejiaRamos2019a-contexts-20210516-1831.feather')\n",
    "contexts = [x for x in df_ramos.columns if x!='arxivid']\n",
    "for c in contexts:\n",
    "    df_ramos[c] = df_ramos[c].apply(flatten)\n",
    "df_ramos = df_ramos.set_index('arxivid')\n",
    "df_ramos.index.names = ['id'] # ArXiv id as index \n",
    "df_ramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv['maincat'].replace({'cs.IT': 'math.IT', 'math-ph': 'math.MP'}, inplace=True)\n",
    "df_arxiv = df_arxiv.drop(df_arxiv[df_arxiv.maincat != df_arxiv.mainmathcat].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv_ramos = pd.read_feather(r'corpus-MejiaRamos2019a-metadata.feather')\n",
    "df_arxiv_ramos['maincat'].replace({'cs.IT': 'math.IT', 'math-ph': 'math.MP'}, inplace=True)\n",
    "df_arxiv_ramos.drop(df_arxiv_ramos[df_arxiv_ramos.maincat != df_arxiv_ramos.mainmathcat].index, inplace=True) \n",
    "\n",
    "\n",
    "df_ramos = pd.read_feather(r'corpus-MejiaRamos2019a-contexts-20210516-1831.feather')\n",
    "contexts = [x for x in df_ramos.columns if x!='arxivid']\n",
    "for c in contexts:\n",
    "    df_ramos[c] = df_ramos[c].apply(flatten)\n",
    "df_ramos = df_ramos.set_index('arxivid')\n",
    "df_ramos.index.names = ['id'] # ArXiv id as index \n",
    "\n",
    "article_list = df_arxiv_ramos.arxivid.tolist()\n",
    "df_ramos = df_ramos[df_ramos.index.isin(article_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count for each context to the data frame for each article\n",
    "contextwordcount_df = [] # Counting words in each context per article\n",
    "for index in tqdm(range(len(df_ramos.index))):\n",
    "    counts_article = []\n",
    "    for c in contexts:\n",
    "        count = len(nltk.word_tokenize(df_ramos[c][index]))\n",
    "        counts_article.append(count)\n",
    "    contextwordcount_df.append({'id':df_ramos.index[index], # Non functional as I list each context\n",
    "                                'outer': counts_article[0], 'theorem': counts_article[1], \n",
    "                                'meta': counts_article[2], 'proof': counts_article[3], \n",
    "                                'other': counts_article[4]})\n",
    "contextwordcount_df = pd.concat([pd.DataFrame(contextwordcount_df[i], index=['id']) for i in range(len(contextwordcount_df))]) \n",
    "contextwordcount_df = contextwordcount_df.set_index('id')\n",
    "# merge to df_arxiv\n",
    "df_arxiv_ramos.set_index('arxivid', inplace=True)\n",
    "df_arxiv_ramos.index.names = ['id']\n",
    "df_arxiv_ramos = df_arxiv_ramos.merge(contextwordcount_df, left_index=True, right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv_ramos.reset_index().to_feather(path+'/ramos2009a_arxiv_extended_database'+timestr+'.feather')\n",
    "df_ramos.reset_index().to_feather(path+'/ramos2009a_arxiv_contexts'+timestr+'.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_cat2 = []\n",
    "for i in df_arxiv_ramos['mainmathcat'].unique().tolist():\n",
    "    numb_cat2.append([i,len(df_arxiv_ramos[df_arxiv_ramos['mainmathcat']==i])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
